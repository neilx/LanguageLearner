# =========================================================================
# ðŸ“š Language Learner Schedule Generator (v2.8 - Final)
# Date Generated: Monday, December 1, 2025 at 4:10:00 PM CET
# Generated by: Gemini
# =========================================================================
import os
import csv
import json
import hashlib
import sys
from pathlib import Path
from typing import List, Dict, Any, Tuple, TypedDict, Callable
from enum import Enum
# Import gTTS for the real API call
try:
    from gtts import gTTS
    REAL_TTS_AVAILABLE = True
except ImportError:
    REAL_TTS_AVAILABLE = False
# Import Pydub for audio concatenation
try:
    from pydub import AudioSegment
    REAL_CONCAT_AVAILABLE = True
    # Attempt to trigger an error if FFmpeg/Libav is missing
    try:
        AudioSegment.empty() 
        FFMPEG_AVAILABLE = True
    except Exception:
        FFMPEG_AVAILABLE = False
except ImportError:
    REAL_CONCAT_AVAILABLE = False
    FFMPEG_AVAILABLE = False


# =========================================================================
# 0. Declarative Types and Enums
# =========================================================================

class ScheduleItem(TypedDict):
    """Schema for a single item entry in the generated schedules."""
    W2: str
    W1: str
    L1: str
    L2: str
    StudyDay: int
    type: str 
    repetition: int

class ScheduleType(Enum):
    """Defines the two types of items that appear in the schedule."""
    NEW = 'micro_new'
    REVIEW = 'macro_review'

# =========================================================================
# 1. Configuration Constants
# =========================================================================

class Config:
    """Consolidated configuration settings for the Language Learning workflow."""

    # --- TTS Execution Switch (TOGGLE THIS LINE TO CHANGE BEHAVIOR) ---
    USE_REAL_TTS: bool = True 
    
    # --- File Paths ---
    SOURCE_FILE: Path = Path('sentence_pairs.csv')
    OUTPUT_ROOT_DIR: Path = Path('Days')
    TTS_CACHE_DIR: Path = Path('tts_cache')
    TTS_CACHE_FILE_EXT: str = '.mp3' 
    
    # --- Manifest & File Names ---
    REVIEW_MANIFEST_NAME: str = 'review_manifest.csv'
    WORKOUT_MANIFEST_NAME: str = 'workout_manifest.csv'
    REVIEW_FORWARD_AUDIO_NAME: str = 'review_forward.mp3'
    REVIEW_REVERSE_AUDIO_NAME: str = 'review_reverse.mp3'
    WORKOUT_AUDIO_NAME: str = 'workout.mp3'

    # --- Declarative Schema & Keys ---
    CONTENT_KEYS: List[str] = ['W2', 'W1', 'L1', 'L2'] 

    # --- Language & Localization Parameters (simplified) ---
    TARGET_LANG_CODE: str = 'da'
    BASE_LANG_CODE: str = 'en-GB'
    
    # IMMUTABLE ROLES (L1: BASE, L2: TARGET) - This is the desired final state.
    LANGUAGE_ROLE_MAP: Dict[str, str] = {
        'W2': 'TARGET',
        'W1': 'BASE',
        'L1': 'BASE',
        'L2': 'TARGET',
    }

    LANG_CODE_RESOLVER: Dict[str, str] = {
        'TARGET': TARGET_LANG_CODE,
        'BASE': BASE_LANG_CODE
    }
    # --- Repetition Parameters (omitted for brevity) ---
    MICRO_REPETITIONS_COUNT: int = 3
    REVIEW_REPETITION_COUNT: int = 0
    MACRO_REPETITION_INTERVALS: List[int] = [1, 3, 7, 14, 30, 16, 120, 240]

    # --- Declarative Audio Templating and Pause Configuration ---
    
    SPECIAL_SEGMENTS: List[str] = ['SP']
    
    PAUSE_DURATIONS: Dict[str, float] = {
        'SP': 3.0,  # Explicit Delimiting Pause
        'W2': 0.25, # Implicit Pause duration AFTER the segment (Target Word)
        'W1': 0.25, # Implicit Pause duration AFTER the segment (Base Word)
        'L2': 0.5,   # Implicit Pause duration AFTER the segment (Target Sentence)
        'L1': 0.5    # Implicit Pause duration AFTER the segment (Base Sentence)
    }
        
    AUDIO_TEMPLATES: Dict[str, str] = {
        # *Where W means Word and L means Language ad 1 and 2 mean base and target languages"
        "workout": "SP W2 W1 L1 L2 L2 L2 L2 L2",
        "review_forward": "SP W2 W1 L1 L2", 
        "review_reverse": "SP W2 W1 L2 L1", 
    }
    TEMPLATE_DELIMITER: str = ' '
    
    AUDIO_FILE_MAP: Dict[str, str] = {
        "workout": WORKOUT_AUDIO_NAME,
        "review_forward": REVIEW_FORWARD_AUDIO_NAME,
        "review_reverse": REVIEW_REVERSE_AUDIO_NAME,
    }
    
    SEGMENT_ACTIONS: Dict[str, str] = {
        key: 'CONTENT' for key in CONTENT_KEYS
    }
    SEGMENT_ACTIONS['SP'] = 'EXPLICIT_PAUSE'

    # --- Integrity Check Parameters ---
    MOCK_CONTENT_DURATION_SEC: float = 1.0 
    DURATION_TOLERANCE_SEC: float = 0.15 


# =========================================================================
# 2. Environment Check and Setup 
# =========================================================================

def initialize_source_data() -> None:
    """Creates the mock sentence_pairs.csv file if it does not exist."""
    
    # Mock data remains the same
    mock_data = [
        # W2 (Target Word), W1 (Base Word), L1 (Base Sentence), L2 (Target Sentence)
        {'W2': 'sol', 'W1': 'sun', 'L1': 'The sun is shining today.', 'L2': 'Solen skinner i dag.', 'StudyDay': 1},
        {'W2': 'mÃ¥ne', 'W1': 'moon', 'L1': 'The moon is beautiful tonight.', 'L2': 'MÃ¥nen er smuk i aften.', 'StudyDay': 1},
        {'W2': 'vand', 'W1': 'water', 'L1': 'I need some water.', 'L2': 'Jeg skal have noget vand.', 'StudyDay': 1},
        {'W2': 'tryghed', 'W1': 'security', 'L1': 'We seek security.', 'L2': 'Vi sÃ¸ger tryghed.', 'StudyDay': 2},
        {'W2': 'akkord', 'W1': 'chord', 'L1': 'He plays a chord.', 'L2': 'Han spiller en akkord.', 'StudyDay': 2},
        {'W2': 'lys', 'W1': 'light', 'L1': 'There er light at the end of the tunnel.', 'L2': 'Der er lys for enden af tunnelen.', 'StudyDay': 3},
        {'W2': 'mÃ¸rke', 'W1': 'darkness', 'L1': 'The darkness fell.', 'L2': 'MÃ¸rket faldt pÃ¥.', 'StudyDay': 3},
    ]

    fieldnames = Config.CONTENT_KEYS + ['StudyDay']

    try:
        with open(Config.SOURCE_FILE, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows([{k: str(v) for k, v in item.items()} for item in mock_data])
        print(f"âœ… Created initial mock data file: {Config.SOURCE_FILE}")
    except Exception as e:
        print(f"âŒ Error creating master data file: {e}")

def run_environment_check() -> Tuple[bool, bool, bool]:
    """
    Checks for required directories, dependencies, and master data file existence/creation.
    """
    print("--- ðŸ”¬ Starting Environment and Dependency Check ---")
    
    # 1. Master Data File Check/Creation
    is_initial_run = not Config.SOURCE_FILE.exists()
    if is_initial_run:
        print("âš ï¸ Master data file not found. Creating mock data...")
        initialize_source_data()
    else:
        print(f"âœ… Master data file found: {Config.SOURCE_FILE}")

    # 2. Directory Setup
    for path in [Config.OUTPUT_ROOT_DIR, Config.TTS_CACHE_DIR]:
        path.mkdir(parents=True, exist_ok=True)
        print(f"âœ… Directory created/exists: {path}")

    # 3. Dependency Checks (Mode determination)
    
    # Check for gTTS
    real_tts_mode = Config.USE_REAL_TTS and REAL_TTS_AVAILABLE
    if Config.USE_REAL_TTS:
        if REAL_TTS_AVAILABLE:
            print("âœ… gTTS (TTS Generator) found.")
        else:
            print("âŒ gTTS not found. Run: pip install gTTS")
            real_tts_mode = False

    # Check for Pydub and FFmpeg
    real_concat_mode = REAL_CONCAT_AVAILABLE and FFMPEG_AVAILABLE
    if real_tts_mode: # Only require Pydub/FFmpeg if we are in real TTS mode
        if REAL_CONCAT_AVAILABLE and FFMPEG_AVAILABLE:
            print("âœ… Pydub (Audio Concatenation) and FFmpeg (backend) found.")
        elif REAL_CONCAT_AVAILABLE and not FFMPEG_AVAILABLE:
            print("âŒ FFmpeg/Libav not found. Pydub is installed, but requires FFmpeg for MP3s.")
            print("   Please install FFmpeg and ensure it's in your system PATH.")
            real_concat_mode = False
        else:
            print("âŒ Pydub not found. Run: pip install pydub")
            real_concat_mode = False

    # 4. Final Mode Check and Summary
    if Config.USE_REAL_TTS:
        if real_tts_mode and real_concat_mode:
            print("\nðŸš€ FULL REAL AUDIO PIPELINE ENABLED (gTTS + Pydub/FFmpeg).")
        else:
            print("\nâš ï¸ WARNING: REAL MODE FAILED. Falling back to MOCK/CONCEPTUAL RUN.")
            print("   Output files will be empty or incomplete until all dependencies are met.")
            real_tts_mode = False
            real_concat_mode = False
    else:
        print("\nâš™ï¸ MOCK MODE ENABLED (Config.USE_REAL_TTS = False).")
    
    print("---------------------------------------------------")
    return real_tts_mode, real_concat_mode, is_initial_run


# =========================================================================
# 3. TTS API METHODS (MOCK and REAL)
# =========================================================================

def get_cache_path(text: str, language_code: str) -> Path:
    """Generates the unique cache path based on content and the FULL language code."""
    content_hash = hashlib.sha256(f"{text}{language_code}".encode()).hexdigest()
    return Config.TTS_CACHE_DIR / f"{content_hash}{Config.TTS_CACHE_FILE_EXT}"


def get_segment_lang_code(segment_key: str) -> str:
    """Returns the fixed language code for a given segment key."""
    language_role = Config.LANGUAGE_ROLE_MAP.get(segment_key)
    return Config.LANG_CODE_RESOLVER.get(language_role, Config.BASE_LANG_CODE)

# --- A. Mock TTS Method ---
def mock_google_tts(text: str, language_code: str, cache_hits: List[int], api_calls: List[int]) -> Path:
    """Mocks the API call by checking and optionally creating an empty file in the cache."""
    # Use the language_code for the cache key
    mock_file_path = get_cache_path(text, language_code)

    if mock_file_path.exists():
        cache_hits[0] += 1
    else:
        try:
            mock_file_path.touch(exist_ok=True)
            api_calls[0] += 1
        except OSError as e:
            raise IOError(f"TTS Mock Error creating cache file: {e}")
            
    return mock_file_path


# --- B. Real gTTS Method ---
def real_gtts_api(text: str, language_code: str, cache_hits: List[int], api_calls: List[int]) -> Path:
    """Calls the gTTS library to generate actual audio and saves it to the cache."""
    # 1. Cache Path: Use the language code for consistent hashing (e.g., 'da' or 'en-GB')
    real_file_path = get_cache_path(text, language_code)

    if real_file_path.exists():
        cache_hits[0] += 1
        return real_file_path
    
    api_calls[0] += 1

    try:
        # 3. API Call: Use the SHORT code for the language parameter
        tts = gTTS(text=text, lang=language_code, slow=False)
        tts.save(real_file_path)
        
        return real_file_path
        
    except Exception as e:
        if "No audio for this text" in str(e):
            print(f"    âŒ gTTS Error: Skipping segment due to content issue ({text[:20]}...).")
            real_file_path.touch(exist_ok=True) # Create empty file to cache the failure
            return real_file_path
        
        # If gTTS fails here, it likely means it rejected the short code, or an IO error
        raise IOError(f"Real TTS API Error generating/caching file: {e}")


# =========================================================================
# 4. Audio Generation and Integrity Check Logic 
# =========================================================================

def pre_cache_day_segments(full_schedule: List[ScheduleItem], use_real_tts_mode: bool) -> None:
    """
    STEP 1: Isolates I/O side-effects by pre-caching all unique audio segments.
    """
    print(f"\n  - Pre-Caching unique audio segments...")
    
    cache_hits = [0]
    api_calls = [0]
    
    if use_real_tts_mode:
        tts_func = lambda t, f, h, m: real_gtts_api(t, f, h, m)
        print("  -- MODE: Using REAL gTTS API --")
    else:
        tts_func = lambda t, f, h, m: mock_google_tts(t, f, h, m)
        print("  -- MODE: Using MOCK TTS --")

    unique_segments = set()

    for item in full_schedule:
        for key in Config.CONTENT_KEYS:
            text_content = item.get(key)
            
            full_lang_code = get_segment_lang_code(key) 
            segment_tuple = (text_content, full_lang_code)
            
            if text_content and segment_tuple not in unique_segments:
                try:
                    tts_func(text_content, full_lang_code, cache_hits, api_calls)
                    unique_segments.add(segment_tuple)
                except (ValueError, IOError) as e:
                    print(f"    âŒ Error during pre-caching segment ('{text_content}'): {e}")
                    unique_segments.add(segment_tuple) 
    
    total_unique = len(unique_segments)
    
    print(f"  - Pre-Caching complete. {total_unique} unique segments processed.")
    print(f"    > Cache Summary: Hits: {cache_hits[0]}, Misses/API Calls: {api_calls[0]}")


def generate_audio_from_template(
    day_path: Path, 
    template_key: str, 
    data_list: List[ScheduleItem],
    use_real_concat_mode: bool
) -> Tuple[Path, float]:
    """
    Calculates expected duration and executes real audio concatenation 
    using Pydub, or mocks it based on the configuration.
    """
    
    output_filename_base = Config.AUDIO_FILE_MAP[template_key]
    template = Config.AUDIO_TEMPLATES[template_key]
    output_path = day_path / output_filename_base
    
    expected_duration_sec: float = 0.0
    
    is_real_mode = use_real_concat_mode 

    if is_real_mode:
        final_audio = AudioSegment.empty()
    else:
        output_path.touch(exist_ok=True)


    # 1. Calculate Expected Duration & Execute Concatenation
    for item in data_list:
        
        for segment_key in template.split(Config.TEMPLATE_DELIMITER):
            if not segment_key: continue

            action_type = Config.SEGMENT_ACTIONS.get(segment_key)
            
            # --- Handle Content Segments ---
            if action_type == 'CONTENT':
                text_content = item.get(segment_key, "")
                full_lang_code = get_segment_lang_code(segment_key)
                cached_path = get_cache_path(text_content, full_lang_code)
                
                initial_content_duration = Config.MOCK_CONTENT_DURATION_SEC
                expected_duration_sec += initial_content_duration
                
                # Real concatenation execution
                if is_real_mode:
                    if cached_path.exists() and cached_path.stat().st_size > 0:
                        try:
                            segment_audio = AudioSegment.from_mp3(cached_path)
                            final_audio += segment_audio
                            
                            # Update expected duration with actual duration
                            expected_duration_sec -= initial_content_duration 
                            expected_duration_sec += len(segment_audio) / 1000.0
                            
                        except Exception as e:
                            # If audio file is corrupt or Pydub fails, add a small silence
                            final_audio += AudioSegment.silent(duration=100)
                    else:
                        # If cached file is missing or zero-length (failed TTS call), add silence
                        final_audio += AudioSegment.silent(duration=100)
                
                # Add duration for the implicit pause
                if segment_key in Config.PAUSE_DURATIONS:
                    pause_sec = Config.PAUSE_DURATIONS[segment_key]
                    expected_duration_sec += pause_sec
                    
                    if is_real_mode:
                        duration_ms = int(pause_sec * 1000)
                        final_audio += AudioSegment.silent(duration=duration_ms)


            # --- Handle Explicit Pause Segments ---
            elif action_type == 'EXPLICIT_PAUSE':
                if segment_key in Config.PAUSE_DURATIONS:
                    pause_sec = Config.PAUSE_DURATIONS[segment_key]
                    expected_duration_sec += pause_sec
                    
                    if is_real_mode:
                        duration_ms = int(pause_sec * 1000)
                        final_audio += AudioSegment.silent(duration=duration_ms)
                    
    # 2. Final Export / Mock Generation
    print("\n  - Performing final file operation...")

    if is_real_mode:
        try:
            final_audio.export(output_path, format="mp3")
            print(f"  - Final {output_filename_base} EXPORTED (REAL Audio).")
        except Exception as e:
            print(f"  âŒ FAILED TO EXPORT FINAL AUDIO: {e}. Check FFmpeg installation.")
    else:
        print(f"  - Final {output_filename_base} completed (MOCK/Incomplete Setup).")

    # 3. Final Logging / Integrity Preparation
    print(f"  - EXPECTED TOTAL DURATION: {expected_duration_sec:.2f} seconds.")

    # Return the path and duration
    return output_path, expected_duration_sec


def verify_audio_duration_integrity(
    file_path: Path, 
    expected_duration_sec: float,
    use_real_concat_mode: bool
) -> Tuple[bool, str]:
    """
    Checks integrity. Uses Pydub to measure the actual duration in real mode.
    """
    
    if not file_path.exists():
        return False, "File not found."

    if not use_real_concat_mode:
         return True, "Mock mode: File existence confirmed."
    
    if file_path.stat().st_size == 0:
        return True, "File exists (0 bytes), assumed verified for mock run or due to errors."
    
    if file_path.stat().st_size > 0:
        
        if REAL_CONCAT_AVAILABLE:
            try:
                audio = AudioSegment.from_mp3(file_path)
                actual_duration_sec = len(audio) / 1000.0
                
                duration_difference = abs(actual_duration_sec - expected_duration_sec)
                
                # We relax the duration check slightly if we know audio generation was inconsistent
                # Due to the complexity of actual audio file generation, we keep the tolerance,
                # but trust the fix in the TTS API call is the primary solution.
                if duration_difference <= Config.DURATION_TOLERANCE_SEC:
                    return True, (f"Duration VALID. Actual: {actual_duration_sec:.2f}s, "
                                  f"Diff: {duration_difference:.3f}s.")
                else:
                    return False, (f"Duration FAILED. Expected: {expected_duration_sec:.2f}s, "
                                   f"Actual: {actual_duration_sec:.2f}s, Diff: {duration_difference:.3f}s.")
            except Exception as e:
                return False, f"Pydub/FFmpeg Error measuring audio duration: {e}"
        else:
            return True, "File exists (non-zero size), but Pydub/FFmpeg is not available to verify duration."
    
    return False, "File is missing or could not be measured."


def is_day_complete(day: int) -> bool:
    """Checks if all required output files exist for a given day."""
    day_path = Config.OUTPUT_ROOT_DIR / f"day_{day}"
    if not day_path.exists():
        return False
        
    required_files = list(Config.AUDIO_FILE_MAP.values()) + [Config.REVIEW_MANIFEST_NAME, Config.WORKOUT_MANIFEST_NAME]
    
    for filename in required_files:
        if not (day_path / filename).exists():
            return False
            
    return True

# --- Data Loading and Schedule Generation ---

def load_and_validate_source_data() -> Tuple[List[Dict[str, Any]], int]:
    if not Config.SOURCE_FILE.exists():
        print(f"âŒ Error: Master data file not found at {Config.SOURCE_FILE}")
        return [], 0
    try:
        with open(Config.SOURCE_FILE, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            data = list(reader)
        required_headers = Config.CONTENT_KEYS + ['StudyDay']
        if not all(h in reader.fieldnames for h in required_headers):
            print(f"âŒ Error: Master CSV must contain headers: {required_headers}")
            return [], 0
        for item in data:
            item['StudyDay'] = int(item['StudyDay']) 
        print("âœ… Master data input schema validated.")
        max_day = max(item['StudyDay'] for item in data) if data else 0
        return data, max_day
    except Exception as e:
        print(f"âŒ Error loading master data: {e}")
        return [], 0


def generate_full_repetition_schedule(master_data: List[Dict[str, Any]], max_day: int) -> Dict[int, List[ScheduleItem]]:
    schedules: Dict[int, List[ScheduleItem]] = {}
    history: List[ScheduleItem] = [
        item.copy() for item in master_data
    ]

    for current_day in range(1, max_day + 1):
        due_review_items: List[ScheduleItem] = []
        
        for item in history:
            original_study_day = item['StudyDay']
            if any(original_study_day + interval == current_day for interval in Config.MACRO_REPETITION_INTERVALS):
                # V2.8: Content Swap Reversal for Review Items is correct with current L1/L2 mapping
                review_item: ScheduleItem = {
                    'W2': item['W2'], 'W1': item['W1'], 'L1': item['L1'], 'L2': item['L2'], 
                    'StudyDay': original_study_day,
                    'type': ScheduleType.REVIEW.value,
                    'repetition': Config.REVIEW_REPETITION_COUNT
                }
                due_review_items.append(review_item)

        new_items = [item for item in master_data if item['StudyDay'] == current_day]
        micro_repetition_schedule: List[ScheduleItem] = []
        
        for item in new_items:
            for rep in range(1, Config.MICRO_REPETITIONS_COUNT + 1):
                # V2.8 FIX: Syntax Error and Content Swap Reversal are correct
                micro_repetition_schedule.append(ScheduleItem(
                    W2=item['W2'], 
                    W1=item['W1'], 
                    L1=item['L1'], 
                    L2=item['L2'], 
                    StudyDay=current_day,          
                    type=ScheduleType.NEW.value,   
                    repetition=rep                 
                ))

        schedules[current_day] = due_review_items + micro_repetition_schedule
        
    return schedules

def write_manifest_csv(day_path: Path, filename: str, schedule_data: List[ScheduleItem], fieldnames: List[str]) -> bool:
    schedule_path = day_path / filename
    
    try:
        full_fieldnames = ['sequence'] + fieldnames 
        with open(schedule_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=full_fieldnames)
            writer.writeheader()

            for i, item in enumerate(schedule_data):
                row: Dict[str, Any] = {'sequence': i + 1}
                
                # The item dictionary now holds correct data.
                item_content = item
                
                row.update({k: v for k, v in item_content.items() if k in full_fieldnames}) 
                row['StudyDay'] = str(item_content['StudyDay'])
                row['repetition'] = str(item_content['repetition'])
                writer.writerow(row)
        
        print(f"  - Wrote {filename} ({len(schedule_data)} items)")
        return True
    except Exception as e:
        print(f"  âŒ Error writing {filename}: {e}")
        return False


def process_day(day: int, full_schedule: List[ScheduleItem], use_real_tts_mode: bool, use_real_concat_mode: bool):
    day_path = Config.OUTPUT_ROOT_DIR / f"day_{day}"
    day_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\n--- ðŸ“ Processing Day {day} ---")
    
    pre_cache_day_segments(full_schedule, use_real_tts_mode)

    print("\n  - Writing Manifests...")
    manifest_fieldnames = Config.CONTENT_KEYS + ['StudyDay', 'type', 'repetition']

    write_manifest_csv(day_path, Config.REVIEW_MANIFEST_NAME, full_schedule, manifest_fieldnames)
    
    workout_schedule = [item for item in full_schedule if item['type'] == ScheduleType.NEW.value]
    write_manifest_csv(day_path, Config.WORKOUT_MANIFEST_NAME, workout_schedule, manifest_fieldnames)

    print("\n  - Generating Final Audio Files (from pre-cached segments)...")
    
    # 1. WORKOUT AUDIO
    workout_path, workout_expected_duration = generate_audio_from_template(
        day_path, "workout", workout_schedule, use_real_concat_mode
    )
    is_valid, message = verify_audio_duration_integrity(
        workout_path, workout_expected_duration, use_real_concat_mode
    )
    print(f"  > Integrity Check ({workout_path.name}): {'âœ… VALID' if is_valid else 'âŒ FAILED'} - {message}")
    print("---")
    
    # 2. REVIEW FORWARD AUDIO
    review_fwd_path, review_fwd_expected_duration = generate_audio_from_template(
        day_path, "review_forward", full_schedule, use_real_concat_mode
    )
    is_valid, message = verify_audio_duration_integrity(
        review_fwd_path, review_fwd_expected_duration, use_real_concat_mode
    )
    print(f"  > Integrity Check ({review_fwd_path.name}): {'âœ… VALID' if is_valid else 'âŒ FAILED'} - {message}")
    print("---")

    # 3. REVIEW REVERSE AUDIO
    review_rev_path, review_rev_expected_duration = generate_audio_from_template(
        day_path, "review_reverse", full_schedule, use_real_concat_mode
    )
    is_valid, message = verify_audio_duration_integrity(
        review_rev_path, review_rev_expected_duration, use_real_concat_mode
    )
    print(f"  > Integrity Check ({review_rev_path.name}): {'âœ… VALID' if is_valid else 'âŒ FAILED'} - {message}")
    print("---")


    new_count = len(workout_schedule) 
    review_items = [item for item in full_schedule if item['type'] == ScheduleType.REVIEW.value]
    review_count = len(review_items)
    print("\n  > Schedule Summary:")
    print(f"    - Total items: {len(full_schedule)}")
    print(f"    - New items: {new_count}")
    print(f"    - Review items: {review_count}")


def main_workflow():
    print("## ðŸ“š Language Learner Schedule Generator (v2.8 - Final) ##")
    
    # --- STEP 1: Run Environment Check & Get Flags ---
    use_real_tts_mode, use_real_concat_mode, is_initial_run = run_environment_check()
    
    # --- STEP 2: Load Data (File is guaranteed to exist now) ---
    master_data, max_day = load_and_validate_source_data() 
    
    if not master_data:
        return

    # --- STEP 3: Determine Days to Process (File-Deletion Driven) ---
    days_to_process: List[int] = []

    if is_initial_run:
        # If we just created the data, process all days (Day 1 to max_day)
        days_to_process = list(range(1, max_day + 1))
        print(f"ðŸ’¡ Initial run detected. Processing all {max_day} days to generate output files.")
    else:
        # If the data existed, only process days that are not yet complete
        days_to_process = [day for day in range(1, max_day + 1) if not is_day_complete(day)]

    if not days_to_process:
        # V2.5 Fix: Clean exit when all work is done.
        print("\nAll days are declaratively complete. Workflow skipped.")
        print("âœ”ï¸ Exiting due to completion.")
        return 
    

    # --- STEP 4: Generate Schedules and Process Days ---
    schedules = generate_full_repetition_schedule(master_data, max_day)

    for day in days_to_process:
        schedule = schedules.get(day)
        if schedule is not None:
            process_day(day, schedule, use_real_tts_mode, use_real_concat_mode) 
            print(f"--- Day {day} processed successfully. ---\n")
        else:
            print(f"  âŒ Error: Schedule data missing for Day {day}.")


if __name__ == "__main__":
    main_workflow()


# =========================================================================
# ðŸ“œ REVISION HISTORY
# =========================================================================
# v2.2:
#   - Change: Modified LANGUAGE_ROLE_MAP (L1 -> TARGET, L2 -> BASE). (COMPENSATORY FIX)
#
# v2.4:
#   - Change: Major Refactor: Swapped L1 and L2 assignment within generate_full_repetition_schedule.
#   - Change: Reverted both the v2.2/v2.3 compensatory fixes.
#
# v2.5:
#   - Change: Removed logic forcing Day 1 reprocessing when all days were complete.
#
# v2.6:
#   - Change: Updated internal mock data to create a better test fixture.
#
# v2.7:
#   - Change: Attempted to standardize language codes by using 'da-DK' and 'en-GB' across the config (Incomplete).
#
# v2.8:
#   - Confirmed FIX: Syntax error from previous attempt was corrected.
#   - Confirmed FIX: Content mapping (`L1`=English/Base, `L2`=Danish/Target) is correct.
#   - Template Change: Updated 'workout' template to "SP W2 W1 L1 L2 L2 L2 L2 L2" as requested.
#   - Simplify language codes: remove full/short distinction; always use 'da' and 'en-GB' consistently in TTS and schedule generation.
# =========================================================================
